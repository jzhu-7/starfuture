import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import csv
import re
import time
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

# =========================
# åŸºæœ¬é…ç½®
# =========================
BASE_URL = "https://bjjs.zjw.beijing.gov.cn"
TARGET_URL = (
    "http://bjjs.zjw.beijing.gov.cn/eportal/ui?"
    "pageId=411612&systemId=2&srcId=1&id=8017587&rowcount=16"
)

def fetch_html(url: str) -> str:
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0 Safari/537.36"
        )
    }
    resp = requests.get(url, headers=headers, timeout=10)
    resp.encoding = resp.apparent_encoding
    return resp.text

def get_buildings_url() -> str:
    html = fetch_html(TARGET_URL)
    soup = BeautifulSoup(html, "html.parser")

    buildings = {}

    for a in soup.find_all("a", href=True):
        href = a["href"]
        name = a.get_text(strip=True)

        # åªç­›é€‰â€œæ¥¼æ ‹é“¾æ¥â€
        if (
            "pageId=320833" in href
            and "buildingId=" in href
            and "salePermitId=" in href
            and name.endswith("ä½å®…æ¥¼")
        ):
            full_url = urljoin(BASE_URL, href)
            full_url = full_url.replace("https://", "http://", 1)
            buildings[name] = full_url

    return buildings

BUILDING_URLS = get_buildings_url()


HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64)"
}

OUTPUT_FILE = "data/areas/areas.json"  # Change output file to .json


# =========================
# 1ï¸âƒ£ æå–æ¥¼ç›˜è¡¨é¡µä¸­çš„æˆ¿å·é“¾æ¥
# =========================
def extract_house_links(html):
    soup = BeautifulSoup(html, "html.parser")
    houses = []

    for a in soup.find_all("a", href=True):
        href = a["href"]
        # æˆ¿å·è¯¦æƒ…é¡µçš„å›ºå®šç‰¹å¾
        if "pageId=373432" in href and "houseId=" in href:
            house_no = a.get_text(strip=True)
            full_url = urljoin(BASE_URL, href)
            full_url = full_url.replace("https://", "http://", 1)
            houses.append({
                "house_no": house_no,
                "url": full_url
            })

    return houses


# =========================
# 2ï¸âƒ£ æå–å•å…ƒå·è¯¦æƒ…é¡µä¸­çš„å»ºç­‘é¢ç§¯
# =========================
def extract_build_area(html):
    soup = BeautifulSoup(html, "html.parser")

    for tr in soup.find_all("tr"):
        tds = tr.find_all("td")
        if len(tds) == 2:
            left = tds[0].get_text(strip=True)
            if left == "å»ºç­‘é¢ç§¯":
                right = tds[1].get_text(strip=True)
                m = re.search(r"([\d.]+)", right)
                if m:
                    return float(m.group(1))

    return None


# =========================
# 3ï¸âƒ£ å¤„ç†å•ä¸ªæ¥¼æ ‹çš„æˆ¿æºä¿¡æ¯
# =========================
def process_building_data(bid, url):
    print(f"ğŸŒ æ­£åœ¨è¯·æ±‚æ¥¼ç›˜è¡¨é¡µé¢{bid} :{url}...")
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.encoding = "utf-8"

        houses = extract_house_links(resp.text)
        print(f"ğŸ  å…±æ‰¾åˆ° {len(houses)} å¥—æˆ¿æº")

        building_data = []

        for idx, h in enumerate(houses, 1):
            print(f"[{idx}/{len(houses)}] è§£æ {h['house_no']} ...")
            try:
                r = requests.get(h["url"], headers=HEADERS, timeout=10)
                r.encoding = "utf-8"
                area = extract_build_area(r.text)
                print(f"  å»ºç­‘é¢ç§¯: {area} å¹³æ–¹ç±³")
                if area is None:
                    print(f"âŒ æœªæ‰¾åˆ°å»ºç­‘é¢ç§¯")
                    continue

                building_data.append({
                    "house_no": h["house_no"],
                    "area": area
                })

                time.sleep(0.3)  # é˜²æ­¢è¯·æ±‚è¿‡å¿«
            except Exception as e:
                print(f"âŒ {h['house_no']} è§£æå¤±è´¥ï¼š{e}")

        return bid, building_data
    except Exception as e:
        print(f"âŒ è¯·æ±‚æ¥¼ç›˜é¡µé¢å¤±è´¥ï¼š{e}")
        return bid, []


# =========================
# 4ï¸âƒ£ ä¸»æµç¨‹
# =========================
def main():
    data = {}

    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(process_building_data, bid, url): bid for bid, url in BUILDING_URLS.items()}
        
        for future in as_completed(futures):
            bid, building_data = future.result()

            if building_data:
                data[bid] = {
                    "building_name": bid,
                    "house_data": building_data
                }

    # =========================
    # 5ï¸âƒ£ å¯¼å‡º JSON
    # =========================
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

    print(f"âœ… å·²å¯¼å‡ºæ•°æ®åˆ° {OUTPUT_FILE}")


if __name__ == "__main__":
    main()
