import requests
from bs4 import BeautifulSoup
import csv
from collections import Counter
import time
from datetime import datetime
import os
import re
from urllib.parse import urljoin
import concurrent.futures
import json

# è‡ªåŠ¨è¯†åˆ«æœ€æ–°çš„ä¸¤ä¸ªæ—¥æœŸæ–‡ä»¶å¤¹
def get_latest_dirs():
    dirs = [d for d in os.listdir('.') if os.path.isdir(d) and re.match(r'\d{4}-\d{2}-\d{2}', d)]
    dirs.sort()
    if len(dirs) < 2:
        raise ValueError("è‡³å°‘éœ€è¦ä¸¤ä¸ªæ—¥æœŸæ–‡ä»¶å¤¹")
    return dirs[-2], dirs[-1]

# é…ç½®è·¯å¾„

def compare_status_changes():
    PREV_DIR, CURR_DIR = get_latest_dirs()

    changes = []

    # éå†å½“å‰ç›®å½•çš„CSVæ–‡ä»¶ï¼ˆæ’é™¤æ±‡æ€»æ–‡ä»¶ï¼‰
    for file in os.listdir(CURR_DIR):
        if not file.endswith('.csv') or 'æ±‡æ€»' in file:
            continue

        curr_path = os.path.join(CURR_DIR, file)
        prev_path = os.path.join(PREV_DIR, file)

        if not os.path.exists(prev_path):
            print(f"è·³è¿‡ {file}ï¼šå‰ä¸€å¤©æ–‡ä»¶ä¸å­˜åœ¨")
            continue

        # è¯»å–å‰ä¸€å¤©æ•°æ®
        prev_data = {}
        with open(prev_path, newline='', encoding='utf-8-sig') as f:
            for row in csv.DictReader(f):
                prev_data[row['house_no']] = row['status']

        # è¯»å–å½“å¤©æ•°æ®å¹¶æ¯”è¾ƒ
        with open(curr_path, newline='', encoding='utf-8-sig') as f:
            for row in csv.DictReader(f):
                house_no = row['house_no']
                curr_status = row['status']
                prev_status = prev_data.get(house_no, 'ä¸å­˜åœ¨')

                if curr_status != prev_status:
                    changes.append({
                        'building': file.replace('.csv', ''),
                        'house_no': house_no,
                        'prev_status': prev_status,
                        'curr_status': curr_status
                    })
    # print(changes)
    return changes

# ==================================================
# æ¥¼æ ‹ URL æ˜ å°„
# ==================================================
BASE_URL = "https://bjjs.zjw.beijing.gov.cn"
TARGET_URL = (
    "http://bjjs.zjw.beijing.gov.cn/eportal/ui?"
    "pageId=411612&systemId=2&srcId=1&id=8017587&rowcount=16"
)

def fetch_html(url: str) -> str:
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0 Safari/537.36"
        )
    }
    resp = requests.get(url, headers=headers, timeout=10)
    resp.encoding = resp.apparent_encoding
    return resp.text

def get_buildings_url() -> str:
    html = fetch_html(TARGET_URL)
    soup = BeautifulSoup(html, "html.parser")

    buildings = {}

    for a in soup.find_all("a", href=True):
        href = a["href"]
        name = a.get_text(strip=True)

        # åªç­›é€‰â€œæ¥¼æ ‹é“¾æ¥â€
        if (
            "pageId=320833" in href
            and "buildingId=" in href
            and "salePermitId=" in href
            and name.endswith("ä½å®…æ¥¼")
        ):
            full_url = urljoin(BASE_URL, href)
            full_url = full_url.replace("https://", "http://", 1)
            buildings[name] = full_url

    return buildings

BUILDING_URLS = get_buildings_url()

# ==================================================
# åŸºæœ¬é…ç½®
# ==================================================
HEADERS = {
    "User-Agent": "Mozilla/5.0"
}

COLOR_STATUS_MAP = {
    "#CCCCCC": "ä¸å¯å”®",
    "#33CC00": "å¯å”®",
    "#FFCC99": "å·²é¢„è®¢",
    "#FF0000": "å·²ç­¾çº¦",
    "#FFFF00": "å·²åŠç†é¢„å”®é¡¹ç›®æŠµæŠ¼",
    "#D2691E": "ç½‘ä¸Šè”æœºå¤‡æ¡ˆ",
    "#00FFFF": "èµ„æ ¼æ ¸éªŒä¸­",
}

ALL_STATUS = list(COLOR_STATUS_MAP.values())

# ==================================================
# å·¥å…·å‡½æ•°
# ==================================================
def parse_status(style: str) -> str:
    style = style.upper()
    for color, status in COLOR_STATUS_MAP.items():
        if color in style:
            return status
    return "å…¶ä»–"

def extract_building_name(soup: BeautifulSoup) -> str:
    span = soup.find("span", string=lambda x: x and "æ¥¼ç›˜è¡¨" in x)
    if span:
        return span.get_text(strip=True).replace("æ¥¼ç›˜è¡¨", "")
    return "æœªçŸ¥æ¥¼æ ‹"

def safe_filename(name: str) -> str:
    return re.sub(r'[\\/:*?"<>|]', "_", name)

# ==================================================
# å¹¶è¡Œå¤„ç†å‡½æ•°
# ==================================================
def process_building(bid, url):
    print(f"\nå¤„ç†æ¥¼æ ‹ {bid}...")

    try:
        resp = requests.get(url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
        resp.encoding = "utf-8"
    except Exception as e:
        print(f"  âŒ è¯·æ±‚å¤±è´¥ï¼š{e}")
        return None, Counter(), 0, "å¤±è´¥"

    soup = BeautifulSoup(resp.text, "html.parser")
    table = soup.find("table", id="table_Buileing")
    if not table:
        print("  âŒ æœªæ‰¾åˆ° table_Buileing")
        return None, Counter(), 0, "å¤±è´¥"

    rows = []
    counter = Counter()

    for div in table.find_all("div"):
        style = div.get("style", "")
        if "BACKGROUND" not in style.upper():
            continue

        a = div.find("a")
        if not a:
            continue

        house_no = a.get_text(strip=True)
        status = parse_status(style)

        rows.append({
            "house_no": house_no,
            "status": status
        })

        counter[status] += 1

    # å•æ ‹æ•°æ®
    building_data = {
        "building_name": bid,
        "house_data": rows,
        "status_count": counter
    }

    return building_data

# ==================================================
# ä¸»æµç¨‹
# ==================================================
def get_status_changes():
    today = datetime.now().strftime("%Y-%m-%d")
    os.makedirs(today, exist_ok=True)

    # æ¥¼æ ‹çº§ç»Ÿè®¡
    all_buildings_data = {}

    # å¹¶è¡Œå¤„ç†æ¥¼æ ‹
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(process_building, bid, url) for bid, url in BUILDING_URLS.items()]
        for future in concurrent.futures.as_completed(futures):
            building_data = future.result()
            if building_data is None:
                continue
            all_buildings_data[building_data["building_name"]] = building_data

    # ä¿å­˜æ•°æ®åˆ° JSON æ–‡ä»¶
    json_path = os.path.join("data", f"{today}.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(all_buildings_data, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“„ å·²ç”Ÿæˆï¼š{json_path}")

    # è·å–çŠ¶æ€å˜åŒ–
    change = compare_status_changes()
    return change

# ==================================================
# å…¥å£
# ==================================================
if __name__ == "__main__":
    get_status_changes()
