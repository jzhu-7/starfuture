import requests
from bs4 import BeautifulSoup
import csv
from collections import Counter
import time
from datetime import datetime
import os
import re
from urllib.parse import urljoin
import concurrent.futures
import json

# è‡ªåŠ¨è¯†åˆ«æœ€æ–°çš„ä¸¤ä¸ªJSONæ–‡ä»¶
def get_latest_json_files():
    sales_dir = "data/sales"
    if not os.path.exists(sales_dir):
        raise ValueError("data/sales ç›®å½•ä¸å­˜åœ¨")
    files = [f for f in os.listdir(sales_dir) if f.endswith('.json') and re.match(r'\d{4}-\d{2}-\d{2}\.json', f)]
    files.sort()
    if len(files) < 2:
        raise ValueError("è‡³å°‘éœ€è¦ä¸¤ä¸ªJSONæ–‡ä»¶")
    return os.path.join(sales_dir, files[-2]), os.path.join(sales_dir, files[-1])

# é…ç½®è·¯å¾„

def compare_status_changes():
    prev_file, curr_file = get_latest_json_files()

    changes = []

    # è¯»å–å‰ä¸€å¤©æ•°æ®
    with open(prev_file, 'r', encoding='utf-8') as f:
        prev_data = json.load(f)

    # è¯»å–å½“å¤©æ•°æ®
    with open(curr_file, 'r', encoding='utf-8') as f:
        curr_data = json.load(f)

    # æ¯”è¾ƒæ¯ä¸ªæ¥¼æ ‹
    for building_name in curr_data:
        if building_name not in prev_data:
            print(f"è·³è¿‡ {building_name}ï¼šå‰ä¸€å¤©æ•°æ®ä¸å­˜åœ¨")
            continue

        prev_building = prev_data[building_name]
        curr_building = curr_data[building_name]

        prev_houses = {h['house_no']: h['status'] for h in prev_building['house_data']}
        curr_houses = curr_building['house_data']

        for house in curr_houses:
            house_no = house['house_no']
            curr_status = house['status']
            prev_status = prev_houses.get(house_no, 'ä¸å­˜åœ¨')

            if curr_status != prev_status:
                changes.append({
                    'building_name': building_name,
                    'house_no': house_no,
                    'prev_status': prev_status,
                    'curr_status': curr_status
                })
    print(changes)
    return changes

# ==================================================
# æ¥¼æ ‹ URL æ˜ å°„
# ==================================================
BASE_URL = "https://bjjs.zjw.beijing.gov.cn"
TARGET_URL = (
    "http://bjjs.zjw.beijing.gov.cn/eportal/ui?"
    "pageId=411612&systemId=2&srcId=1&id=8017587&rowcount=16"
)

def fetch_html(url: str) -> str:
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0 Safari/537.36"
        )
    }
    resp = requests.get(url, headers=headers, timeout=10)
    resp.encoding = resp.apparent_encoding
    return resp.text

def get_buildings_url() -> str:
    html = fetch_html(TARGET_URL)
    soup = BeautifulSoup(html, "html.parser")

    buildings = {}

    for a in soup.find_all("a", href=True):
        href = a["href"]
        name = a.get_text(strip=True)

        # åªç­›é€‰â€œæ¥¼æ ‹é“¾æ¥â€
        if (
            "pageId=320833" in href
            and "buildingId=" in href
            and "salePermitId=" in href
            and name.endswith("ä½å®…æ¥¼")
        ):
            full_url = urljoin(BASE_URL, href)
            full_url = full_url.replace("https://", "http://", 1)
            buildings[name] = full_url

    return buildings

BUILDING_URLS = get_buildings_url()

# ==================================================
# åŸºæœ¬é…ç½®
# ==================================================
HEADERS = {
    "User-Agent": "Mozilla/5.0"
}

COLOR_STATUS_MAP = {
    "#CCCCCC": "ä¸å¯å”®",
    "#33CC00": "å¯å”®",
    "#FFCC99": "å·²é¢„è®¢",
    "#FF0000": "å·²ç­¾çº¦",
    "#FFFF00": "å·²åŠç†é¢„å”®é¡¹ç›®æŠµæŠ¼",
    "#D2691E": "ç½‘ä¸Šè”æœºå¤‡æ¡ˆ",
    "#00FFFF": "èµ„æ ¼æ ¸éªŒä¸­",
}

ALL_STATUS = list(COLOR_STATUS_MAP.values())

# ==================================================
# å·¥å…·å‡½æ•°
# ==================================================
def parse_status(style: str) -> str:
    style = style.upper()
    for color, status in COLOR_STATUS_MAP.items():
        if color in style:
            return status
    return "å…¶ä»–"

def extract_building_name(soup: BeautifulSoup) -> str:
    span = soup.find("span", string=lambda x: x and "æ¥¼ç›˜è¡¨" in x)
    if span:
        return span.get_text(strip=True).replace("æ¥¼ç›˜è¡¨", "")
    return "æœªçŸ¥æ¥¼æ ‹"

def safe_filename(name: str) -> str:
    return re.sub(r'[\\/:*?"<>|]', "_", name)

# ==================================================
# å¹¶è¡Œå¤„ç†å‡½æ•°
# ==================================================
def process_building(bid, url):
    print(f"\nå¤„ç†æ¥¼æ ‹ {bid}...")

    try:
        resp = requests.get(url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
        resp.encoding = "utf-8"
    except Exception as e:
        print(f"  âŒ è¯·æ±‚å¤±è´¥ï¼š{e}")
        return None, Counter(), 0, "å¤±è´¥"

    soup = BeautifulSoup(resp.text, "html.parser")
    table = soup.find("table", id="table_Buileing")
    if not table:
        print("  âŒ æœªæ‰¾åˆ° table_Buileing")
        return None, Counter(), 0, "å¤±è´¥"

    rows = []
    counter = Counter()

    for div in table.find_all("div"):
        style = div.get("style", "")
        if "BACKGROUND" not in style.upper():
            continue

        a = div.find("a")
        if not a:
            continue

        house_no = a.get_text(strip=True)
        status = parse_status(style)

        rows.append({
            "house_no": house_no,
            "status": status
        })

        counter[status] += 1

    # å•æ ‹æ•°æ®
    building_data = {
        "building_name": bid,
        "house_data": rows,
        "status_count": counter
    }

    return building_data

# ==================================================
# ä¸»æµç¨‹
# ==================================================
def get_status_changes():
    today = datetime.now().strftime("%Y-%m-%d")
    os.makedirs(today, exist_ok=True)

    # æ¥¼æ ‹çº§ç»Ÿè®¡
    all_buildings_data = {}

    # å¹¶è¡Œå¤„ç†æ¥¼æ ‹
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(process_building, bid, url) for bid, url in BUILDING_URLS.items()]
        for future in concurrent.futures.as_completed(futures):
            building_data = future.result()
            if building_data is None:
                continue
            all_buildings_data[building_data["building_name"]] = building_data

    # ä¿å­˜æ•°æ®åˆ° JSON æ–‡ä»¶
    json_path = os.path.join("data","sales", f"{today}.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(all_buildings_data, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“„ å·²ç”Ÿæˆï¼š{json_path}")

    # è·å–çŠ¶æ€å˜åŒ–
    change = compare_status_changes()
    return change

# ==================================================
# å…¥å£
# ==================================================
if __name__ == "__main__":
    get_status_changes()
