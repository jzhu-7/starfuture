"""
çŠ¶æ€æ•°æ®æŠ“å–æ¨¡å—
è´Ÿè´£æŠ“å–æˆ¿å±‹é”€å”®çŠ¶æ€ä¿¡æ¯
"""
import os
import json
import re
import requests
import logging
from datetime import datetime
from collections import Counter
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Tuple, Optional
from bs4 import BeautifulSoup

from config import HEADERS, SALES_DIR, COLOR_STATUS_MAP, MAX_WORKERS
from utils import fetch_html, get_buildings_url
from models import HouseData, BuildingData, StatusChange

logger = logging.getLogger(__name__)

def parse_status(style: str) -> str:
    """è§£æçŠ¶æ€æ ·å¼"""
    style = style.upper()
    for color, status in COLOR_STATUS_MAP.items():
        if color in style:
            return status
    return "å…¶ä»–"

def extract_building_name(soup) -> str:
    """æå–æ¥¼æ ‹åç§°"""
    span = soup.find("span", string=lambda x: x and "æ¥¼ç›˜è¡¨" in x)
    if span:
        return span.get_text(strip=True).replace("æ¥¼ç›˜è¡¨", "")
    return "æœªçŸ¥æ¥¼æ ‹"

def process_building(bid: str, url: str) -> Optional[BuildingData]:
    """å¤„ç†å•ä¸ªæ¥¼æ ‹"""
    logger.info(f"å¤„ç†æ¥¼æ ‹ {bid}...")

    try:
        resp = requests.get(url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
        resp.encoding = "utf-8"
    except Exception as e:
        logger.error(f"  âŒ è¯·æ±‚å¤±è´¥ï¼š{e}")
        return None

    soup = BeautifulSoup(resp.text, "html.parser")
    table = soup.find("table", id="table_Buileing")
    if not table:
        logger.error("  âŒ æœªæ‰¾åˆ° table_Buileing")
        return None

    rows = []
    counter = Counter()

    for div in table.find_all("div"):
        style = div.get("style", "")
        if "BACKGROUND" not in style.upper():
            continue

        a = div.find("a")
        if not a:
            continue

        house_no = a.get_text(strip=True)
        status = parse_status(style)

        rows.append(HouseData(
            house_no=house_no,
            status=status
        ))

        counter[status] += 1

    return BuildingData(
        building_name=bid,
        house_data=rows,
        status_count=dict(counter)
    )

def scrape_status_data() -> Dict[str, BuildingData]:
    """æŠ“å–æ‰€æœ‰æ¥¼æ ‹çŠ¶æ€æ•°æ®"""
    BUILDING_URLS = get_buildings_url()
    all_buildings_data = {}

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(process_building, bid, url)
                  for bid, url in BUILDING_URLS.items()]
        for future in as_completed(futures):
            building_data = future.result()
            if building_data:
                all_buildings_data[building_data.building_name] = building_data

    return all_buildings_data

def save_status_data(data: Dict[str, BuildingData], date: str):
    """ä¿å­˜çŠ¶æ€æ•°æ®åˆ°æ–‡ä»¶"""
    os.makedirs(SALES_DIR, exist_ok=True)
    json_path = os.path.join(SALES_DIR, f"{date}.json")

    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    dict_data = {}
    for bid, bdata in data.items():
        dict_data[bid] = {
            "building_name": bdata.building_name,
            "house_data": [{"house_no": h.house_no, "status": h.status} for h in bdata.house_data],
            "status_count": bdata.status_count
        }

    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(dict_data, f, ensure_ascii=False, indent=2)

    logger.info(f"ğŸ“„ å·²ç”Ÿæˆï¼š{json_path}")
    return json_path

def compare_status_changes(prev_file: str, curr_file: str) -> List[StatusChange]:
    """æ¯”è¾ƒçŠ¶æ€å˜åŒ–"""
    changes = []

    # è¯»å–å‰ä¸€å¤©æ•°æ®
    with open(prev_file, 'r', encoding='utf-8') as f:
        prev_data = json.load(f)

    # è¯»å–å½“å¤©æ•°æ®
    with open(curr_file, 'r', encoding='utf-8') as f:
        curr_data = json.load(f)

    # æ¯”è¾ƒæ¯ä¸ªæ¥¼æ ‹
    for building_name in curr_data:
        if building_name not in prev_data:
            logger.warning(f"è·³è¿‡ {building_name}ï¼šå‰ä¸€å¤©æ•°æ®ä¸å­˜åœ¨")
            continue

        prev_building = prev_data[building_name]
        curr_building = curr_data[building_name]

        prev_houses = {h['house_no']: h['status'] for h in prev_building['house_data']}
        curr_houses = curr_building['house_data']

        for house in curr_houses:
            house_no = house['house_no']
            curr_status = house['status']
            prev_status = prev_houses.get(house_no, 'ä¸å­˜åœ¨')

            if curr_status != prev_status:
                changes.append(StatusChange(
                    building_name=building_name,
                    house_no=house_no,
                    prev_status=prev_status,
                    curr_status=curr_status
                ))

    return changes

def get_latest_json_files() -> Tuple[str, str]:
    """è·å–æœ€æ–°çš„ä¸¤ä¸ªJSONæ–‡ä»¶"""
    if not os.path.exists(SALES_DIR):
        raise ValueError("data/sales ç›®å½•ä¸å­˜åœ¨")
    files = [f for f in os.listdir(SALES_DIR) if f.endswith('.json') and re.match(r'\d{4}-\d{2}-\d{2}\.json', f)]
    files.sort()
    if len(files) < 2:
        raise ValueError("è‡³å°‘éœ€è¦ä¸¤ä¸ªJSONæ–‡ä»¶")
    return os.path.join(SALES_DIR, files[-2]), os.path.join(SALES_DIR, files[-1])

def get_status_changes() -> List[StatusChange]:
    """è·å–çŠ¶æ€å˜åŒ–ï¼ˆå®Œæ•´æµç¨‹ï¼‰"""
    today = datetime.now().strftime("%Y-%m-%d")

    # æŠ“å–å¹¶ä¿å­˜å½“å¤©æ•°æ®
    status_data = scrape_status_data()
    save_status_data(status_data, today)

    # æ¯”è¾ƒçŠ¶æ€å˜åŒ–
    try:
        prev_file, curr_file = get_latest_json_files()
        changes = compare_status_changes(prev_file, curr_file)
        return changes
    except ValueError:
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®ï¼Œè¿”å›ç©ºåˆ—è¡¨
        return []