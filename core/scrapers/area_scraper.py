"""
é¢ç§¯æ•°æ®æŠ“å–æ¨¡å—
è´Ÿè´£æŠ“å–æ¥¼æ ‹å’Œæˆ¿æºé¢ç§¯ä¿¡æ¯
"""
import json
import re
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional, Tuple
from bs4 import BeautifulSoup
from urllib.parse import urljoin

from config import HEADERS, BASE_URL
from utils import fetch_html, get_buildings_url, safe_delay
from models import HouseData, BuildingData

def extract_house_links(html: str) -> List[Dict]:
    """æå–æˆ¿å·é“¾æ¥"""
    soup = BeautifulSoup(html, "html.parser")
    houses = []

    for a in soup.find_all("a", href=True):
        href = a["href"]
        # æˆ¿å·è¯¦æƒ…é¡µçš„å›ºå®šç‰¹å¾
        if "pageId=373432" in href and "houseId=" in href:
            house_no = a.get_text(strip=True)
            full_url = urljoin(BASE_URL, href)
            full_url = full_url.replace("https://", "http://", 1)
            houses.append({
                "house_no": house_no,
                "url": full_url
            })

    return houses

def extract_build_area(html: str) -> Optional[float]:
    """æå–å»ºç­‘é¢ç§¯"""
    soup = BeautifulSoup(html, "html.parser")

    for tr in soup.find_all("tr"):
        tds = tr.find_all("td")
        if len(tds) == 2:
            left = tds[0].get_text(strip=True)
            if left == "å»ºç­‘é¢ç§¯":
                right = tds[1].get_text(strip=True)
                m = re.search(r"([\d.]+)", right)
                if m:
                    return float(m.group(1))

    return None

def process_building_data(bid: str, url: str) -> Tuple[str, List[HouseData]]:
    """å¤„ç†å•ä¸ªæ¥¼æ ‹çš„æˆ¿æºä¿¡æ¯"""
    print(f"ğŸŒ æ­£åœ¨è¯·æ±‚æ¥¼ç›˜è¡¨é¡µé¢{bid} :{url}...")
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.encoding = "utf-8"

        houses = extract_house_links(resp.text)
        print(f"ğŸ  å…±æ‰¾åˆ° {len(houses)} å¥—æˆ¿æº")

        building_data = []

        for idx, h in enumerate(houses, 1):
            print(f"[{idx}/{len(houses)}] è§£æ {h['house_no']} ...")
            try:
                r = requests.get(h["url"], headers=HEADERS, timeout=10)
                r.encoding = "utf-8"
                area = extract_build_area(r.text)
                print(f"  å»ºç­‘é¢ç§¯: {area} å¹³æ–¹ç±³")
                if area is None:
                    print(f"âŒ æœªæ‰¾åˆ°å»ºç­‘é¢ç§¯")
                    continue

                building_data.append(HouseData(
                    house_no=h["house_no"],
                    area=area
                ))

                safe_delay()  # é˜²æ­¢è¯·æ±‚è¿‡å¿«
            except Exception as e:
                print(f"âŒ {h['house_no']} è§£æå¤±è´¥ï¼š{e}")

        return bid, building_data
    except Exception as e:
        print(f"âŒ è¯·æ±‚æ¥¼ç›˜é¡µé¢å¤±è´¥ï¼š{e}")
        return bid, []

def scrape_areas_data(output_file: str = "data/areas/areas.json") -> Dict[str, BuildingData]:
    """ä¸»æµç¨‹ï¼šæŠ“å–æ‰€æœ‰æ¥¼æ ‹é¢ç§¯æ•°æ®"""
    BUILDING_URLS = get_buildings_url()
    data = {}

    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(process_building_data, bid, url): bid
                  for bid, url in BUILDING_URLS.items()}

        for future in as_completed(futures):
            bid, building_data = future.result()

            if building_data:
                data[bid] = BuildingData(
                    building_name=bid,
                    house_data=building_data
                )

    # å¯¼å‡º JSON
    with open(output_file, "w", encoding="utf-8") as f:
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
        dict_data = {}
        for bid, bdata in data.items():
            dict_data[bid] = {
                "building_name": bdata.building_name,
                "house_data": [{"house_no": h.house_no, "area": h.area} for h in bdata.house_data]
            }
        json.dump(dict_data, f, ensure_ascii=False, indent=4)

    print(f"âœ… å·²å¯¼å‡ºæ•°æ®åˆ° {output_file}")
    return data